{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Common commands**"
      ],
      "metadata": {
        "id": "KgM-4ixPPjss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('path_to_your_model')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model('path_to_your_model')"
      ],
      "metadata": {
        "id": "WOLtBU4irZrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To delete a directory\n",
        "import shutil\n",
        "\n",
        "# Define the directory to delete\n",
        "directory_to_delete = 'PATH'\n",
        "\n",
        "# Use shutil.rmtree() to delete the directory\n",
        "shutil.rmtree(directory_to_delete)\n",
        "\n",
        "print(\"Directory and its contents deleted successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU7iWc-SPQhH",
        "outputId": "589560c1-5a3f-4364-b977-527e9c22b6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory and its contents deleted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to move a directory\n",
        "import shutil\n",
        "\n",
        "source_dir = '/path/to/source_directory'\n",
        "destination_dir = '/path/to/destination_directory'\n",
        "\n",
        "# Move the directory\n",
        "shutil.move(source_dir, destination_dir)\n",
        "\n",
        "print(\"Directory moved successfully.\")\n"
      ],
      "metadata": {
        "id": "hdRB9twYbwgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To know how many files are in the directory\n",
        "import os\n",
        "\n",
        "directory = 'Path'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)\n"
      ],
      "metadata": {
        "id": "KXc2pTrhR7YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define directories\n",
        "source_dir = 'path'\n",
        "destination_dir = 'path'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Define parameters\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "batch_size = 32\n",
        "\n",
        "# Create ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    channel_shift_range=50,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load images from the source directory\n",
        "image_files = os.listdir(source_dir)\n",
        "\n",
        "# Augment and save the images\n",
        "for filename in image_files:\n",
        "    try:\n",
        "        img = cv2.imread(os.path.join(source_dir, filename))\n",
        "        if img is None:\n",
        "            print(f\"Failed to load image: {filename}\")\n",
        "            continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB format\n",
        "\n",
        "        # Generate augmented images\n",
        "        i = 0\n",
        "        img = img.reshape((1,) + img.shape)  # Reshape image for the flow method\n",
        "        for batch in datagen.flow(img, batch_size=batch_size, save_to_dir=destination_dir, save_prefix='aug', save_format='jpg'):\n",
        "            i += 1\n",
        "            if i >= 30:  # Generate 30 augmented images per original image\n",
        "                break  # Exit the loop to move to the next image\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {filename}: {e}\")\n",
        "\n",
        "print(\"Augmentation completed.\")\n"
      ],
      "metadata": {
        "id": "i7N-ZQV2bRLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PIPE DETECTION**"
      ],
      "metadata": {
        "id": "u_1AXoh6Jw4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DuPPmj7m-FY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b2601f-44e5-4ae7-e07e-0fe901f7653d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define the directories\n",
        "dataset_dir = '/path/to/labeled_dataset'\n",
        "train_dir = '/path/to/training_set'\n",
        "val_dir = '/path/to/validation_set'\n",
        "test_dir = '/path/to/testing_set'\n",
        "\n",
        "# Define the labels (folder names)\n",
        "labels = ['pipes', 'not_pipe']\n",
        "\n",
        "# Define the ratios for splitting\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for directory in [train_dir, val_dir, test_dir]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    for label in labels:\n",
        "        os.makedirs(os.path.join(directory, label), exist_ok=True)\n",
        "\n",
        "# Iterate through each label directory and split the images\n",
        "for label in labels:\n",
        "    label_dir = os.path.join(dataset_dir, label)\n",
        "    images = os.listdir(label_dir)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    num_images = len(images)\n",
        "    train_split = int(train_ratio * num_images)\n",
        "    val_split = int((train_ratio + val_ratio) * num_images)\n",
        "\n",
        "    train_images = images[:train_split]\n",
        "    val_images = images[train_split:val_split]\n",
        "    test_images = images[val_split:]\n",
        "\n",
        "    for img in train_images:\n",
        "        src = os.path.join(label_dir, img)\n",
        "        dst = os.path.join(train_dir, label, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    for img in val_images:\n",
        "        src = os.path.join(label_dir, img)\n",
        "        dst = os.path.join(val_dir, label, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    for img in test_images:\n",
        "        src = os.path.join(label_dir, img)\n",
        "        dst = os.path.join(test_dir, label, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "print(\"Dataset split completed.\")\n"
      ],
      "metadata": {
        "id": "kVl6sG2z5O91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define directories\n",
        "train_dir = '/content/drive/MyDrive/dataset/pipes_dataset/training'\n",
        "val_dir = '/content/drive/MyDrive/dataset/pipes_dataset/validation'\n",
        "\n",
        "# Define parameters\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "# Create ImageDataGenerator for training data with data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create ImageDataGenerator for validation data\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Get the list of classes in the training directory\n",
        "classes = os.listdir(train_dir)\n",
        "\n",
        "# Load and label training data\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load and label validation data, excluding .ipynb_checkpoints\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    classes=classes\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBx4U1Er-YtV",
        "outputId": "9477194e-5942-4ca1-d6cb-a4510bfc2b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 919 images belonging to 2 classes.\n",
            "Found 306 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/dataset/pipes_dataset/training'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqSCeuWmuNXH",
        "outputId": "141fbebd-ac11-43b5-93aa-26c0960b6118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/dataset/pipes_dataset/validation'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF52pH32uYWl",
        "outputId": "fbd8b178-c502-4780-ec0e-2e92eedb1e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/dataset/pipes_dataset/testing'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dms83v_Uuxkq",
        "outputId": "9ea026fd-9510-4a2e-8982-7e0d92ea8b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with increasing depth\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output and add fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary classification, so use sigmoid activation\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Augment training data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/dataset/pipes_dataset/training', target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "# Load validation data\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_generator = val_datagen.flow_from_directory('/content/drive/MyDrive/dataset/pipes_dataset/validation', target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=20, validation_data=val_generator, validation_steps=val_generator.samples // batch_size)\n",
        "\n",
        "# Evaluate the model\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/MyDrive/dataset/pipes_dataset/testing', target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "eval_result = model.evaluate(test_generator)\n",
        "\n",
        "print(\"Test Loss:\", eval_result[0])\n",
        "print(\"Test Accuracy:\", eval_result[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKJUZYsCvqHE",
        "outputId": "8b3cfce2-b523-4d27-c630-2bed274163d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 919 images belonging to 2 classes.\n",
            "Found 306 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "28/28 [==============================] - 226s 8s/step - loss: 0.8255 - accuracy: 0.5648 - val_loss: 0.6038 - val_accuracy: 0.6736\n",
            "Epoch 2/20\n",
            "28/28 [==============================] - 68s 2s/step - loss: 0.6020 - accuracy: 0.6697 - val_loss: 0.5592 - val_accuracy: 0.6875\n",
            "Epoch 3/20\n",
            "28/28 [==============================] - 73s 3s/step - loss: 0.5294 - accuracy: 0.7204 - val_loss: 0.5253 - val_accuracy: 0.7153\n",
            "Epoch 4/20\n",
            "28/28 [==============================] - 69s 2s/step - loss: 0.5098 - accuracy: 0.7463 - val_loss: 0.5124 - val_accuracy: 0.7326\n",
            "Epoch 5/20\n",
            "28/28 [==============================] - 71s 3s/step - loss: 0.5047 - accuracy: 0.7610 - val_loss: 0.4752 - val_accuracy: 0.7743\n",
            "Epoch 6/20\n",
            "28/28 [==============================] - 73s 3s/step - loss: 0.4589 - accuracy: 0.7779 - val_loss: 0.4425 - val_accuracy: 0.7986\n",
            "Epoch 7/20\n",
            "28/28 [==============================] - 79s 3s/step - loss: 0.4237 - accuracy: 0.8061 - val_loss: 0.3915 - val_accuracy: 0.8299\n",
            "Epoch 8/20\n",
            "28/28 [==============================] - 76s 3s/step - loss: 0.3559 - accuracy: 0.8534 - val_loss: 0.3717 - val_accuracy: 0.8576\n",
            "Epoch 9/20\n",
            "28/28 [==============================] - 70s 3s/step - loss: 0.3569 - accuracy: 0.8591 - val_loss: 0.3615 - val_accuracy: 0.8611\n",
            "Epoch 10/20\n",
            "28/28 [==============================] - 65s 2s/step - loss: 0.3164 - accuracy: 0.8715 - val_loss: 0.4244 - val_accuracy: 0.8542\n",
            "Epoch 11/20\n",
            "28/28 [==============================] - 66s 2s/step - loss: 0.3005 - accuracy: 0.8737 - val_loss: 0.3060 - val_accuracy: 0.8611\n",
            "Epoch 12/20\n",
            "28/28 [==============================] - 70s 2s/step - loss: 0.2423 - accuracy: 0.9098 - val_loss: 0.3208 - val_accuracy: 0.8854\n",
            "Epoch 13/20\n",
            "28/28 [==============================] - 71s 3s/step - loss: 0.2569 - accuracy: 0.8895 - val_loss: 0.2998 - val_accuracy: 0.8646\n",
            "Epoch 14/20\n",
            "28/28 [==============================] - 67s 2s/step - loss: 0.2658 - accuracy: 0.8805 - val_loss: 0.3379 - val_accuracy: 0.8750\n",
            "Epoch 15/20\n",
            "28/28 [==============================] - 70s 2s/step - loss: 0.2290 - accuracy: 0.9109 - val_loss: 0.2893 - val_accuracy: 0.8993\n",
            "Epoch 16/20\n",
            "28/28 [==============================] - 70s 2s/step - loss: 0.1820 - accuracy: 0.9312 - val_loss: 0.4376 - val_accuracy: 0.8438\n",
            "Epoch 17/20\n",
            "28/28 [==============================] - 74s 3s/step - loss: 0.1786 - accuracy: 0.9301 - val_loss: 0.2529 - val_accuracy: 0.9132\n",
            "Epoch 18/20\n",
            "28/28 [==============================] - 64s 2s/step - loss: 0.1953 - accuracy: 0.9177 - val_loss: 0.2284 - val_accuracy: 0.9306\n",
            "Epoch 19/20\n",
            "28/28 [==============================] - 70s 2s/step - loss: 0.1394 - accuracy: 0.9459 - val_loss: 0.3917 - val_accuracy: 0.8854\n",
            "Epoch 20/20\n",
            "28/28 [==============================] - 64s 2s/step - loss: 0.1513 - accuracy: 0.9448 - val_loss: 0.2579 - val_accuracy: 0.9306\n",
            "Found 307 images belonging to 2 classes.\n",
            "10/10 [==============================] - 28s 3s/step - loss: 0.1999 - accuracy: 0.9218\n",
            "Test Loss: 0.19993534684181213\n",
            "Test Accuracy: 0.9218240976333618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('/content/drive/MyDrive/dataset/pipe_model/model1.h5')\n"
      ],
      "metadata": {
        "id": "q42Hd2B7D4eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your trained model\n",
        "model = load_model('/content/drive/MyDrive/dataset/pipe_model/model1.h5')  # Replace 'your_model.h5' with the path to your trained model\n",
        "\n",
        "# Function to preprocess input image\n",
        "def preprocess_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(img_height, img_width))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_image(image_path):\n",
        "    img = preprocess_image(image_path)\n",
        "    predictions = model.predict(img)\n",
        "    return predictions\n",
        "\n",
        "# Function to interpret predictions\n",
        "def interpret_predictions(predictions):\n",
        "    class_index = int(predictions[0][0])  # Assuming predictions is of shape (1, 1)\n",
        "    confidence = predictions[0][0] if class_index == 1 else 1 - predictions[0][0]\n",
        "    if class_index == 0:\n",
        "        return \"No pipes are detected in the image\", confidence\n",
        "    elif class_index == 1:\n",
        "        return \"Pipe is detected in the image\", confidence\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/drive/MyDrive/dataset/Figura-1-Ducto-submarino-con-claro-libre-no-soportado-ADSUN-2008vvvvvvvvv.png'  # Replace 'path_to_your_image.jpg' with the path to your image\n",
        "predictions = predict_image(image_path)\n",
        "interpretation, confidence = interpret_predictions(predictions)\n",
        "print(interpretation)\n",
        "print(f\"Confidence: {confidence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26SJQq-AEgXf",
        "outputId": "c169f3fa-91bf-40b2-f018-386e6005dadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 118ms/step\n",
            "Pipe is detected in the image\n",
            "Confidence: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUST DETECTION**"
      ],
      "metadata": {
        "id": "lUOyfLQ8KBMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To know how many files are in the directory\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/dataset/rust_dataset/data/no_rust'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GDbpb14QWYX",
        "outputId": "17a65778-9ed1-4247-bbae-d284d3101ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 1534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define directories\n",
        "source_dir = '/content/drive/MyDrive/dataset/rust_dataset/rusted'\n",
        "destination_dir = '/content/drive/MyDrive/dataset/rust_dataset/data/rust'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Define parameters\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "batch_size = 32\n",
        "\n",
        "# Create ImageDataGenerator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    channel_shift_range=50,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load images from the source directory\n",
        "image_files = os.listdir(source_dir)\n",
        "\n",
        "# Augment and save the images\n",
        "for filename in image_files:\n",
        "    try:\n",
        "        img = cv2.imread(os.path.join(source_dir, filename))\n",
        "        if img is None:\n",
        "            print(f\"Failed to load image: {filename}\")\n",
        "            continue\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB format\n",
        "\n",
        "        # Generate augmented images\n",
        "        i = 0\n",
        "        img = img.reshape((1,) + img.shape)  # Reshape image for the flow method\n",
        "        for batch in datagen.flow(img, batch_size=batch_size, save_to_dir=destination_dir, save_prefix='aug', save_format='jpg'):\n",
        "            i += 1\n",
        "            if i >= 30:  # Generate 30 augmented images per original image\n",
        "                break  # Exit the loop to move to the next image\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {filename}: {e}\")\n",
        "\n",
        "print(\"Augmentation completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ_nflntbOFd",
        "outputId": "ec38c362-e648-48a2-cad2-a4b331582972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmentation completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to move a directory\n",
        "import shutil\n",
        "\n",
        "source_dir = '/content/drive/MyDrive/dataset/rust_dataset/og_data/no_rust/rusted'\n",
        "destination_dir = '/content/drive/MyDrive/dataset/rust_dataset/og_data'\n",
        "\n",
        "# Move the directory\n",
        "shutil.move(source_dir, destination_dir)\n",
        "\n",
        "print(\"Directory moved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-3uEfWhdP-c",
        "outputId": "acf933b8-38ad-4481-fb83-18b91249118e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory moved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to move a directory\n",
        "import shutil\n",
        "\n",
        "source_dir = '/content/drive/MyDrive/dataset/rust_dataset/nonrusted'\n",
        "destination_dir = '/content/drive/MyDrive/dataset/rust_dataset/og_data'\n",
        "\n",
        "# Move the directory\n",
        "shutil.move(source_dir, destination_dir)\n",
        "\n",
        "print(\"Directory moved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhxZa14KeKaI",
        "outputId": "75e55818-21a7-4f76-a4ba-67017f19d3ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory moved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the directories\n",
        "dataset_dir = '/content/drive/MyDrive/dataset/rust_dataset/data'  # Path to your dataset directory\n",
        "labels = ['no_rust', 'rust']  # Define your labels\n",
        "\n",
        "# Create label directories if they don't exist\n",
        "for label in labels:\n",
        "    label_dir = os.path.join(dataset_dir, label)\n",
        "    os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "# Function to move files to corresponding label directories\n",
        "def label_data(file, label):\n",
        "    src = os.path.join(dataset_dir, label, file)\n",
        "    dst = os.path.join(dataset_dir, label, file)\n",
        "    shutil.move(src, dst)\n",
        "\n",
        "# Iterate through the labels (folder names) in the dataset directory\n",
        "for label in labels:\n",
        "    # List all files in the label directory\n",
        "    files = os.listdir(os.path.join(dataset_dir, label))\n",
        "    # Move files to corresponding label directories\n",
        "    for file in files:\n",
        "        label_data(file, label)\n",
        "\n",
        "print(\"Labeling completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUfcmBLlfGjq",
        "outputId": "9dbfdbd4-a099-40e2-f87a-4701f9ac889e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labeling completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Define the directories\n",
        "dataset_dir = '/content/drive/MyDrive/dataset/rust_dataset/data'\n",
        "train_dir = '/content/drive/MyDrive/dataset/rust_dataset/training'\n",
        "val_dir = '/content/drive/MyDrive/dataset/rust_dataset/validation'\n",
        "test_dir = '/content/drive/MyDrive/dataset/rust_dataset/testing'\n",
        "\n",
        "# Define the labels (folder names)\n",
        "labels = ['no_rust', 'rust']\n",
        "\n",
        "# Define the ratios for splitting\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for directory in [train_dir, val_dir, test_dir]:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    for label in labels:\n",
        "        os.makedirs(os.path.join(directory, label), exist_ok=True)\n",
        "\n",
        "# Iterate through each label directory and split the images\n",
        "for label in labels:\n",
        "    label_dir = os.path.join(dataset_dir, label)\n",
        "    images = os.listdir(label_dir)\n",
        "    random.shuffle(images)\n",
        "\n",
        "    num_images = len(images)\n",
        "    train_split = int(train_ratio * num_images)\n",
        "    val_split = int((train_ratio + val_ratio) * num_images)\n",
        "\n",
        "    train_images = images[:train_split]\n",
        "    val_images = images[train_split:val_split]\n",
        "    test_images = images[val_split:]\n",
        "\n",
        "    for img in train_images:\n",
        "        src = os.path.join(label_dir, img)\n",
        "        dst = os.path.join(train_dir, label, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    for img in val_images:\n",
        "        src = os.path.join(label_dir, img)\n",
        "        dst = os.path.join(val_dir, label, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    for img in test_images:\n",
        "        src = os.path.join(label_dir, img)\n",
        "        dst = os.path.join(test_dir, label, img)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "print(\"Dataset split completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_msoHqIMeR1X",
        "outputId": "8c5cbceb-e441-4620-aef5-604b9039af84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset split completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define directories\n",
        "train_dir = '/content/drive/MyDrive/dataset/rust_dataset/training'\n",
        "val_dir = '/content/drive/MyDrive/dataset/rust_dataset/validation'\n",
        "\n",
        "# Define parameters\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "\n",
        "# Create ImageDataGenerator for training data with data augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create ImageDataGenerator for validation data\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Get the list of classes in the training directory\n",
        "classes = os.listdir(train_dir)\n",
        "\n",
        "# Load and label training data\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load and label validation data, excluding .ipynb_checkpoints\n",
        "val_data = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    classes=classes\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAn6QKfTgKdE",
        "outputId": "16e189d7-fde6-4bf9-e58c-f80ec5ba13e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1454 images belonging to 2 classes.\n",
            "Found 485 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To know how many files are in the directory\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/dataset/rust_dataset/testing'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b51f9vWdgikB",
        "outputId": "15d7f96f-b92c-4c78-a1a1-2c4c649791b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To know how many files are in the directory\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/dataset/rust_dataset/training'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN8uAmZhgoi9",
        "outputId": "304c0d63-f76e-4a56-8e8e-b8609f191ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 1454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To know how many files are in the directory\n",
        "import os\n",
        "\n",
        "directory = '/content/drive/MyDrive/dataset/rust_dataset/validation'  # Change this to your directory path\n",
        "file_count = sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "print(\"Number of files in the directory:\", file_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0iADBWxgpeD",
        "outputId": "243db03a-b024-48d6-cd27-89e0cf954a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the directory: 485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_width, img_height = 150, 150\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers with increasing depth\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Flatten the output and add fully connected layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Binary classification, so use sigmoid activation\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Augment training data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/dataset/rust_dataset/training', target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "# Load validation data\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_generator = val_datagen.flow_from_directory('/content/drive/MyDrive/dataset/rust_dataset/validation', target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, steps_per_epoch=train_generator.samples // batch_size, epochs=20, validation_data=val_generator, validation_steps=val_generator.samples // batch_size)\n",
        "\n",
        "# Evaluate the model\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/MyDrive/dataset/rust_dataset/testing', target_size=(img_height, img_width), batch_size=batch_size, class_mode='binary')\n",
        "eval_result = model.evaluate(test_generator)\n",
        "\n",
        "print(\"Test Loss:\", eval_result[0])\n",
        "print(\"Test Accuracy:\", eval_result[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msE1KkCSg7V8",
        "outputId": "28d33060-5791-4fad-dd73-06cb1bdc1e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1454 images belonging to 2 classes.\n",
            "Found 485 images belonging to 2 classes.\n",
            "Epoch 1/20\n",
            "45/45 [==============================] - 110s 2s/step - loss: 0.6778 - accuracy: 0.5999 - val_loss: 0.5935 - val_accuracy: 0.6333\n",
            "Epoch 2/20\n",
            "45/45 [==============================] - 108s 2s/step - loss: 0.5913 - accuracy: 0.6575 - val_loss: 0.5305 - val_accuracy: 0.6979\n",
            "Epoch 3/20\n",
            "45/45 [==============================] - 107s 2s/step - loss: 0.5185 - accuracy: 0.7328 - val_loss: 0.4816 - val_accuracy: 0.7708\n",
            "Epoch 4/20\n",
            "45/45 [==============================] - 109s 2s/step - loss: 0.4752 - accuracy: 0.7630 - val_loss: 0.4686 - val_accuracy: 0.7708\n",
            "Epoch 5/20\n",
            "45/45 [==============================] - 106s 2s/step - loss: 0.4341 - accuracy: 0.7813 - val_loss: 0.4202 - val_accuracy: 0.8000\n",
            "Epoch 6/20\n",
            "45/45 [==============================] - 101s 2s/step - loss: 0.3941 - accuracy: 0.8186 - val_loss: 0.3634 - val_accuracy: 0.8354\n",
            "Epoch 7/20\n",
            "45/45 [==============================] - 107s 2s/step - loss: 0.3725 - accuracy: 0.8200 - val_loss: 0.2995 - val_accuracy: 0.8583\n",
            "Epoch 8/20\n",
            "45/45 [==============================] - 105s 2s/step - loss: 0.3233 - accuracy: 0.8502 - val_loss: 0.3034 - val_accuracy: 0.8646\n",
            "Epoch 9/20\n",
            "45/45 [==============================] - 105s 2s/step - loss: 0.2475 - accuracy: 0.9015 - val_loss: 0.2480 - val_accuracy: 0.8792\n",
            "Epoch 10/20\n",
            "45/45 [==============================] - 108s 2s/step - loss: 0.2221 - accuracy: 0.9079 - val_loss: 0.1794 - val_accuracy: 0.9229\n",
            "Epoch 11/20\n",
            "45/45 [==============================] - 106s 2s/step - loss: 0.1753 - accuracy: 0.9311 - val_loss: 0.1917 - val_accuracy: 0.9187\n",
            "Epoch 12/20\n",
            "45/45 [==============================] - 107s 2s/step - loss: 0.2043 - accuracy: 0.9248 - val_loss: 0.1783 - val_accuracy: 0.9167\n",
            "Epoch 13/20\n",
            "45/45 [==============================] - 107s 2s/step - loss: 0.1181 - accuracy: 0.9536 - val_loss: 0.1086 - val_accuracy: 0.9708\n",
            "Epoch 14/20\n",
            "45/45 [==============================] - 106s 2s/step - loss: 0.1441 - accuracy: 0.9423 - val_loss: 0.1174 - val_accuracy: 0.9542\n",
            "Epoch 15/20\n",
            "45/45 [==============================] - 104s 2s/step - loss: 0.1104 - accuracy: 0.9571 - val_loss: 0.0985 - val_accuracy: 0.9625\n",
            "Epoch 16/20\n",
            "45/45 [==============================] - 102s 2s/step - loss: 0.0671 - accuracy: 0.9782 - val_loss: 0.0725 - val_accuracy: 0.9729\n",
            "Epoch 17/20\n",
            "45/45 [==============================] - 102s 2s/step - loss: 0.0884 - accuracy: 0.9648 - val_loss: 0.0551 - val_accuracy: 0.9875\n",
            "Epoch 18/20\n",
            "45/45 [==============================] - 107s 2s/step - loss: 0.1128 - accuracy: 0.9557 - val_loss: 0.1471 - val_accuracy: 0.9417\n",
            "Epoch 19/20\n",
            "45/45 [==============================] - 107s 2s/step - loss: 0.0897 - accuracy: 0.9712 - val_loss: 0.1085 - val_accuracy: 0.9688\n",
            "Epoch 20/20\n",
            "45/45 [==============================] - 107s 2s/step - loss: 0.0723 - accuracy: 0.9768 - val_loss: 0.0413 - val_accuracy: 0.9917\n",
            "Found 486 images belonging to 2 classes.\n",
            "16/16 [==============================] - 9s 524ms/step - loss: 0.0617 - accuracy: 0.9815\n",
            "Test Loss: 0.06167373061180115\n",
            "Test Accuracy: 0.9814814925193787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('rust_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMmzXuTDqvX9",
        "outputId": "d4a922ea-eede-4406-bc8d-b56ba74874b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if the model file exists\n",
        "model_file = 'rust_model.h5'\n",
        "if os.path.exists(model_file):\n",
        "    print(\"Model saved successfully as\", model_file)\n",
        "else:\n",
        "    print(\"Model saving failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZDrno2CrBdn",
        "outputId": "c8f0a4c8-5550-44cd-c4b7-b827b61b5fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully as rust_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load your trained model\n",
        "model = load_model('/content/rust_model.h5')  # Replace 'your_model.h5' with the path to your trained model\n",
        "\n",
        "# Function to preprocess input image\n",
        "def preprocess_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(img_height, img_width))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n",
        "\n",
        "# Function to make predictions\n",
        "def predict_image(image_path):\n",
        "    img = preprocess_image(image_path)\n",
        "    predictions = model.predict(img)\n",
        "    return predictions\n",
        "\n",
        "# Function to interpret predictions\n",
        "def interpret_predictions(predictions):\n",
        "    class_index = int(predictions[0][0])  # Assuming predictions is of shape (1, 1)\n",
        "    confidence = predictions[0][0] if class_index == 1 else 1 - predictions[0][0]\n",
        "    if class_index == 0:\n",
        "        return \"Not rusted \", confidence\n",
        "    elif class_index == 1:\n",
        "        return \"Rusted pipe is found !\", confidence\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/istockphoto-1062920970-2048x2048.jpg'\n",
        "predictions = predict_image(image_path)\n",
        "interpretation, confidence = interpret_predictions(predictions)\n",
        "print(interpretation)\n",
        "print(f\"Confidence: {confidence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmoptk6VrMNf",
        "outputId": "7dfd5fb9-13a5-465f-9164-ed4a5cc30a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 173ms/step\n",
            "Rusted pipe is found !\n",
            "Confidence: 1.0\n"
          ]
        }
      ]
    }
  ]
}